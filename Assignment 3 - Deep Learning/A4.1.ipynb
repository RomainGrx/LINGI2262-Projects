{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5e3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# I will use my own packet named \"from_scratch\" that is an implementation of a deep learning framework using simple numpy function\n",
    "# You can find all scirpts here : https://github.com/RomainGrx/deep-learning-from-scratch\n",
    "\n",
    "# Add the github repo in the path\n",
    "fs_path = os.path.join(os.getcwd(), \"deep-learning-from-scratch\")\n",
    "if fs_path not in sys.path:\n",
    "    sys.path.append(fs_path)\n",
    "    \n",
    "import from_scratch as fs\n",
    "from from_scratch.models import Sequential\n",
    "from from_scratch import layers, losses, optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf432fef",
   "metadata": {},
   "source": [
    "XOR problem \n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9f52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xor = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_xor = np.array([0, 1, 1, 0]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a5653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+--------------+---------------+\n",
      "|      Layer Name      | Input Shape | Output Shape | Nb Parameters |\n",
      "+----------------------+-------------+--------------+---------------+\n",
      "|        Dense         |  (None, 2)  |  (None, 2)   |       6       |\n",
      "|  Activation (relu)   |  (None, 2)  |  (None, 2)   |       0       |\n",
      "|        Dense         |  (None, 2)  |  (None, 1)   |       3       |\n",
      "| Activation (sigmoid) |  (None, 1)  |  (None, 1)   |       0       |\n",
      "+----------------------+-------------+--------------+---------------+\n",
      "Total parameters : 9\n"
     ]
    }
   ],
   "source": [
    "# Given data \n",
    "Wxh = np.array([[.79, 1.34], [.87, 1.08]])\n",
    "bh = np.array([.10, -1.12])\n",
    "Why = np.array([[.68], [-2.01]])\n",
    "by = np.array([-.3])\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(2, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    losses.Crossentropy(),\n",
    "    optimizers.GradientDescent(lr=0.2),\n",
    ")\n",
    "\n",
    "# Needed to initialize all layers, weights, ...\n",
    "dry_run = model.forward(X_xor)\n",
    "\n",
    "# Fix the weights and biases\n",
    "# layers[0] = x->h layer\n",
    "# layers[1] = the ReLU activation layer\n",
    "# layers[2] = h->y layer\n",
    "# layers[3] = Sigmoid activation layer\n",
    "model.layers[0].weights = Wxh\n",
    "model.layers[0].biases = bh\n",
    "model.layers[2].weights = Why\n",
    "model.layers[2].biases = by\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca5a51",
   "metadata": {},
   "source": [
    "Question 1 : Forward propagation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c52fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputed y_hat ::  0.442, 0.589, 0.466, 0.152\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.forward(X_xor)\n",
    "print(\"Outputed y_hat :: \", \", \".join([\"%.3f\"%v for v in y_hat]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397403b",
   "metadata": {},
   "source": [
    "Question 2 : Classification\n",
    "---\n",
    "\n",
    "How many examples are correctly classified with the current network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a18475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly classified :: 3\n"
     ]
    }
   ],
   "source": [
    "def xor_correct(y_hat):\n",
    "    y_hat_rounded = np.round(y_hat)\n",
    "    return np.sum(y_xor == y_hat_rounded)\n",
    "\n",
    "n_corrects = xor_correct(y_hat)\n",
    "print(\"Number of correctly classified :: %d\"%n_corrects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca8469",
   "metadata": {},
   "source": [
    "Question 3 : Loss\n",
    "---\n",
    "\n",
    "What is the value of the cross-entropy loss $L(\\hat{y}, y)$ computed from the 4 examples using this network? This cross-entropy loss is the cost function $J$ of the network, which depends on the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c4c5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary crossentropy loss :: 2.043\n"
     ]
    }
   ],
   "source": [
    "loss_fn = fs.losses.Crossentropy()\n",
    "loss = np.sum(loss_fn(y_xor, y_hat))\n",
    "print(\"Binary crossentropy loss :: %.3f\"%loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4df6ff",
   "metadata": {},
   "source": [
    "Question 4 : Back-propagation\n",
    "---\n",
    "\n",
    "In order to update the weights of our model, we will back-propagate one example, namely $x_3 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $y_3=1$.\n",
    "\n",
    "First, we need to compute the output layer gradient $\\nabla{\\hat{y}}J$. What is its value when considering this example ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57479284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouput gradient :: -2.147\n"
     ]
    }
   ],
   "source": [
    "x3, y3 = np.array([[1, 0]]), np.array([[1]])\n",
    "\n",
    "y_hat3 = model.forward(x3)\n",
    "loss_grad = loss_fn.gradient(y3, y_hat3)[0,0]\n",
    "print(f'Ouput gradient :: {loss_grad:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09aa1fa",
   "metadata": {},
   "source": [
    "Question 5 : Back-propagation (continued) \n",
    "---\n",
    "\n",
    "Consequently, what are the values of $\\nabla_{W_{h \\rightarrow y}} J$ and $\\nabla_{b_{y}} J$, i.e. the gradients of the weight vectors and the bias of the last layer?\n",
    "\n",
    "Give your answer using the format *grad_w_1*, *grad_w_2*, *grad_b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1b9c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w_1, grad_w2, grad_b :: -0.475, -0.118, -0.534\n"
     ]
    }
   ],
   "source": [
    "sigmoid_grad = model.layers[-1].backward(loss_grad)\n",
    "Why_grad = model.layers[-2].backward(sigmoid_grad, get_all=True)\n",
    "\n",
    "grad_w_hy = Why_grad[\"weights\"]\n",
    "grad_b_hy = Why_grad[\"biases\"]\n",
    "\n",
    "print(f'grad_w_1, grad_w2, grad_b :: {grad_w_hy[0,0]:.3f}, {grad_w_hy[1,0]:.3f}, {grad_b_hy[0,0]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7f49a",
   "metadata": {},
   "source": [
    "Question 6 :  Back-propagation (continued) \n",
    "---\n",
    "\n",
    "Next, we need to back-propagate the gradient to the hidden layer.\n",
    "\n",
    "What is the value of $\\nabla_h J$ ?\n",
    "\n",
    "Give your answer using the format *grad_h_1*, *grad_h_2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa595610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_h_1, grad_h_2 :: -0.363, 1.074\n"
     ]
    }
   ],
   "source": [
    "print(f'grad_h_1, grad_h_2 :: {Why_grad[\"inputs\"][0,0]:.3f}, {Why_grad[\"inputs\"][0,1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf12d8",
   "metadata": {},
   "source": [
    "Question 7 : \n",
    "---\n",
    "\n",
    "What are the values of $\\nabla_{W_{x \\rightarrow h}} J$ and $\\nabla_{b_h} J$ , i.e. the gradients of the weight matrix and the bias vector of the first layer?\n",
    "\n",
    "Give your answer using the format *grad_w_11*, *grad_w_12*, *grad_w_21*, *grad_w_22*, *grad_b_1*, *grad_b_2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563f2b06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1c6d7cf6b391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgrad_w_xh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWxh_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrad_b_xh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWxh_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"biases\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'grad_w_11, grad_w_12, grad_w_21, grad_w_22, grad_b_1, grad_b_2 :: {grad_w[0,0]:.3f}, {grad_w[0,1]:.3f}, {grad_w[1,0]:.3f}, {grad_w[1,1]:.3f}, {grad_b[0,0]:.3f}, {grad_b[0,1]:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_w' is not defined"
     ]
    }
   ],
   "source": [
    "relu_grad = model.layers[-3].backward(Why_grad[\"inputs\"])\n",
    "Wxh_grad = model.layers[-4].backward(relu_grad, get_all=True)\n",
    "\n",
    "grad_w_xh = Wxh_grad[\"weights\"]\n",
    "grad_b_xh = Wxh_grad[\"biases\"]\n",
    "print(f'grad_w_11, grad_w_12, grad_w_21, grad_w_22, grad_b_1, grad_b_2 :: {grad_w[0,0]:.3f}, {grad_w[0,1]:.3f}, {grad_w[1,0]:.3f}, {grad_w[1,1]:.3f}, {grad_b[0,0]:.3f}, {grad_b[0,1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0325cb",
   "metadata": {},
   "source": [
    "Question 8 : Weight update\n",
    "---\n",
    "\n",
    "Using your previous answers, you can update the weights of the neural network. What are the new parameters $W_{h \\rightarrow y}$ and $b_y$\n",
    "\n",
    "of the last layer?\n",
    "\n",
    "Give your answer using the format *w_1*, *w_2*, *b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49141590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The backward pass already update the weights thanks to the optimizer in the model.compile\n",
    "# but we can still calculate from scratch \n",
    "[[w_hy_1], [w_hy_2]] = model.layers[-2].weights\n",
    "[[b_hy]] = model.layers[-2].biases\n",
    "\n",
    "# From scratch with the gradient descent rule \n",
    "[[new_w_hy_1], [new_w_hy_2]] = Why - .2* grad_w_hy\n",
    "[[new_b_hy]] = by - .2* grad_b_hy\n",
    "\n",
    "# Verify we have the same variables\n",
    "assert new_w_hy_1 == w_hy_1\n",
    "assert new_w_hy_2 == w_hy_2\n",
    "assert new_b_hy == b_hy\n",
    "\n",
    "print(f'w_1, w_2, b :: {w_hy_1:.3f}, {w_hy_2:.3f}, {b_hy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d1312",
   "metadata": {},
   "source": [
    "Question 9 : Weight update (continued)\n",
    "---\n",
    "\n",
    "What are the new parameters $W_{x \\rightarrow h}$ and $b_h$ of the first layer?\n",
    "\n",
    "Give your answer using the format *w_11*, *w_12*, *w_21*, *w_22*, *b_1*, *b_2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2935afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = model.layers[0].weights\n",
    "bh = model.layers[0].biases\n",
    "print(f'w_11, w_12, w_21, w_22, b_1, b_2 :: {Wxh[0,0]:.3f}, {Wxh[0,1]:.3f}, {Wxh[1,0]:.3f}, {Wxh[1,1]:.3f}, {bh[0,0]:.3f}, {bh[0,1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b96aa",
   "metadata": {},
   "source": [
    "Question 10 :  Forward propagation \n",
    "---\n",
    "\n",
    "Now, we will evaluate whether our model has been improved thanks to the back-propagation.\n",
    "\n",
    "Propagate each example $x_i$ through the neural network.\n",
    "\n",
    "How are these examples represented in the hidden space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = model.layers[1].forward(model.layers[0].forward(X_xor))\n",
    "y = np.round(model.forward(X_xor)).reshape(-1)\n",
    "\n",
    "zeros = y == 0\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(r\"$h_1$\")\n",
    "plt.ylabel(r\"$h_2$\")\n",
    "plt.scatter(h[zeros,0], h[zeros,1], color=\"purple\", label=\"0\")\n",
    "plt.scatter(h[~zeros,0], h[~zeros,1], color=\"blue\", label=\"1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e6ef4",
   "metadata": {},
   "source": [
    "Question 11: Forward propagation (continued) \n",
    "---\n",
    "\n",
    "Are they linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc3b92",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ada553",
   "metadata": {},
   "source": [
    "Question 12: Forward propagation (continued)\n",
    "---\n",
    "\n",
    "What are the values of the outputs $\\hat{y_i}$?\n",
    "\n",
    "Give your answer using the format y_1, y_2, y_3, y_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae58867",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.forward(X_xor)\n",
    "\n",
    "print(\"y_1, y_2, y_3, y_4 :: \", \", \".join([\"%.4f\"%v for v in y_hat.reshape(-1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce5834",
   "metadata": {},
   "source": [
    "Question 13: Classification \n",
    "---\n",
    "\n",
    "How many examples are correctly classified with the updated network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a374437",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xor_correct(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830185b",
   "metadata": {},
   "source": [
    "Question 14: Loss \n",
    "---\n",
    "\n",
    "What is the cross-entropy loss with the updated network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a231b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.sum(model.loss_function(y_xor, y_hat))\n",
    "print(f\"Loss :: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffacce",
   "metadata": {},
   "source": [
    "Question 15: Conclusion \n",
    "---\n",
    "\n",
    "Check all the valid affirmations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d80bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_xor, y_xor, epochs=1e5, verbose=0)\n",
    "final_y = model.forward(X_xor).reshape(-1)\n",
    "\n",
    "print(\"Final y_hat :: \", \", \".join([\"%.3f\"%v for v in final_y]))\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Crossentropy loss value\")\n",
    "plt.semilogx(history[\"epochs\"], history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e842c8",
   "metadata": {},
   "source": [
    "- [x] The gradient descent algorithm will continue iterating to decrease the loss, until the latter converges\n",
    "- [ ] Since all the examples are correctly classified, the gradient descent algorithm will stop at this point, as the model can no longer be improved\n",
    "- [x] If we let the gradient descent algorithm continue, we could expect it to optimise the network to a point where the output values would be something like $\\hat{y_1}=0.01$, $\\hat{y_2}=0.99$, $\\hat{y_3}=0.99$, $\\hat{y_4}=0.01$  \n",
    "- [ ] The gradient descent algorithm will always converge to the same point, regardless of the initial values of the network parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INGI2262",
   "language": "python",
   "name": "lingi2262"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
