{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07d408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# I will use my own packet named \"from_scratch\" that is an implementation of a deep learning framework using simple numpy function\n",
    "# You can find all scirpts here : https://github.com/RomainGrx/deep-learning-from-scratch\n",
    "\n",
    "# Add the github repo in the path\n",
    "fs_path = os.path.join(os.getcwd(), \"deep-learning-from-scratch\")\n",
    "if fs_path not in sys.path:\n",
    "    sys.path.append(fs_path)\n",
    "    \n",
    "import from_scratch as fs\n",
    "from from_scratch.models import Sequential\n",
    "from from_scratch import layers, losses, optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567fa4",
   "metadata": {},
   "source": [
    "XOR problem \n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8888ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xor = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_xor = np.array([0, 1, 1, 0]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a560dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+--------------+---------------+\n",
      "|      Layer Name      | Input Shape | Output Shape | Nb Parameters |\n",
      "+----------------------+-------------+--------------+---------------+\n",
      "|        Dense         |  (None, 2)  |  (None, 2)   |       6       |\n",
      "|  Activation (relu)   |  (None, 2)  |  (None, 2)   |       0       |\n",
      "|        Dense         |  (None, 2)  |  (None, 1)   |       3       |\n",
      "| Activation (sigmoid) |  (None, 1)  |  (None, 1)   |       0       |\n",
      "+----------------------+-------------+--------------+---------------+\n",
      "Total parameters : 9\n"
     ]
    }
   ],
   "source": [
    "# Given data \n",
    "Wxh = np.array([[.79, 1.34], [.87, 1.08]])\n",
    "bh = np.array([.10, -1.12])\n",
    "Why = np.array([[.68], [-2.01]])\n",
    "by = np.array([-.3])\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(2, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    losses.Crossentropy(),\n",
    "    optimizers.GradientDescent(lr=0.2),\n",
    ")\n",
    "\n",
    "# Needed to initialize all layers, weights, ...\n",
    "dry_run = model.forward(X_xor)\n",
    "\n",
    "# Fix the weights and biases\n",
    "# layers[0] = x->h layer\n",
    "# layers[1] = the ReLU activation layer\n",
    "# layers[2] = h->y layer\n",
    "# layers[3] = Sigmoid activation layer\n",
    "model.layers[0].weights = Wxh\n",
    "model.layers[0].biases = bh\n",
    "model.layers[2].weights = Why\n",
    "model.layers[2].biases = by\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88927f66",
   "metadata": {},
   "source": [
    "Question 1 : Forward propagation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04c4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputed y_hat ::  0.442, 0.589, 0.466, 0.152\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.forward(X_xor)\n",
    "print(\"Outputed y_hat :: \", \", \".join([\"%.3f\"%v for v in y_hat]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9d5d5",
   "metadata": {},
   "source": [
    "Question 2 : Classification\n",
    "---\n",
    "\n",
    "How many examples are correctly classified with the current network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f06810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly classified :: 3\n"
     ]
    }
   ],
   "source": [
    "def xor_correct(y_hat):\n",
    "    y_hat_rounded = np.round(y_hat)\n",
    "    return np.sum(y_xor == y_hat_rounded)\n",
    "\n",
    "n_corrects = xor_correct(y_hat)\n",
    "print(\"Number of correctly classified :: %d\"%n_corrects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e62a5",
   "metadata": {},
   "source": [
    "Question 3 : Loss\n",
    "---\n",
    "\n",
    "What is the value of the cross-entropy loss $L(\\hat{y}, y)$ computed from the 4 examples using this network? This cross-entropy loss is the cost function $J$ of the network, which depends on the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4bf5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary crossentropy loss :: 2.043\n"
     ]
    }
   ],
   "source": [
    "loss_fn = fs.losses.Crossentropy()\n",
    "loss = np.sum(loss_fn(y_xor, y_hat))\n",
    "print(\"Binary crossentropy loss :: %.3f\"%loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363081a",
   "metadata": {},
   "source": [
    "Question 4 : Back-propagation\n",
    "---\n",
    "\n",
    "In order to update the weights of our model, we will back-propagate one example, namely $x_3 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $y_3=1$.\n",
    "\n",
    "First, we need to compute the output layer gradient $\\nabla{\\hat{y}}J$. What is its value when considering this example ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a63915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouput gradient :: -2.147\n"
     ]
    }
   ],
   "source": [
    "x3, y3 = np.array([[1, 0]]), np.array([[1]])\n",
    "\n",
    "y_hat3 = model.forward(x3)\n",
    "loss_grad = loss_fn.gradient(y3, y_hat3)[0,0]\n",
    "print(f'Ouput gradient :: {loss_grad:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb603df",
   "metadata": {},
   "source": [
    "Question 5 : Back-propagation (continued) \n",
    "---\n",
    "\n",
    "Consequently, what are the values of $\\nabla_{W_{h \\rightarrow y}} J$ and $\\nabla_{b_{y}} J$, i.e. the gradients of the weight vectors and the bias of the last layer?\n",
    "\n",
    "Give your answer using the format *grad_w_1*, *grad_w_2*, *grad_b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40933d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w_1, grad_w2, grad_b :: -0.475, -0.118, -0.534\n"
     ]
    }
   ],
   "source": [
    "sigmoid_grad = model.layers[-1].backward(loss_grad)\n",
    "Why_grad = model.layers[-2].backward(sigmoid_grad, get_all=True)\n",
    "\n",
    "grad_w_hy = Why_grad[\"weights\"]\n",
    "grad_b_hy = Why_grad[\"biases\"]\n",
    "\n",
    "print(f'grad_w_1, grad_w2, grad_b :: {grad_w_hy[0,0]:.3f}, {grad_w_hy[1,0]:.3f}, {grad_b_hy[0,0]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef55838",
   "metadata": {},
   "source": [
    "Question 6 :  Back-propagation (continued) \n",
    "---\n",
    "\n",
    "Next, we need to back-propagate the gradient to the hidden layer.\n",
    "\n",
    "What is the value of $\\nabla_h J$ ?\n",
    "\n",
    "Give your answer using the format *grad_h_1*, *grad_h_2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "061b44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_h_1, grad_h_2 :: -0.363, 1.074\n"
     ]
    }
   ],
   "source": [
    "print(f'grad_h_1, grad_h_2 :: {Why_grad[\"inputs\"][0,0]:.3f}, {Why_grad[\"inputs\"][0,1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de3fd9",
   "metadata": {},
   "source": [
    "Question 7 : \n",
    "---\n",
    "\n",
    "What are the values of $\\nabla_{W_{x \\rightarrow h}} J$ and $\\nabla_{b_h} J$ , i.e. the gradients of the weight matrix and the bias vector of the first layer?\n",
    "\n",
    "Give your answer using the format *grad_w_11*, *grad_w_12*, *grad_w_21*, *grad_w_22*, *grad_b_1*, *grad_b_2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7050244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w_xh_11, grad_w_xh_12, grad_w_xh_21, grad_w_xh_22, grad_b_xh_1, grad_b_xh_2 :: -0.363, 1.074, 0.000, 0.000, -0.363, 1.074\n"
     ]
    }
   ],
   "source": [
    "relu_grad = model.layers[-3].backward(Why_grad[\"inputs\"])\n",
    "Wxh_grad = model.layers[-4].backward(relu_grad, get_all=True)\n",
    "\n",
    "grad_w_xh = Wxh_grad[\"weights\"]\n",
    "grad_b_xh = Wxh_grad[\"biases\"]\n",
    "print(f'grad_w_xh_11, grad_w_xh_12, grad_w_xh_21, grad_w_xh_22, grad_b_xh_1, grad_b_xh_2 :: {grad_w_xh[0,0]:.3f}, {grad_w_xh[0,1]:.3f}, {grad_w_xh[1,0]:.3f}, {grad_w_xh[1,1]:.3f}, {grad_b_xh[0,0]:.3f}, {grad_b_xh[0,1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc7ba5",
   "metadata": {},
   "source": [
    "Question 8 : Weight update\n",
    "---\n",
    "\n",
    "Using your previous answers, you can update the weights of the neural network. What are the new parameters $W_{h \\rightarrow y}$ and $b_y$\n",
    "\n",
    "of the last layer?\n",
    "\n",
    "Give your answer using the format *w_1*, *w_2*, *b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7213e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_1, w_2, b :: 0.775, -1.986, -0.193\n"
     ]
    }
   ],
   "source": [
    "# The backward pass already update the weights thanks to the optimizer in the model.compile\n",
    "# but we can still calculate from scratch \n",
    "[[w_hy_1], [w_hy_2]] = model.layers[-2].weights\n",
    "[[b_hy]] = model.layers[-2].biases\n",
    "\n",
    "# From scratch with the gradient descent rule \n",
    "[[new_w_hy_1], [new_w_hy_2]] = Why - .2* grad_w_hy\n",
    "[[new_b_hy]] = by - .2* grad_b_hy\n",
    "\n",
    "# Verify we have the same variables\n",
    "assert new_w_hy_1 == w_hy_1\n",
    "assert new_w_hy_2 == w_hy_2\n",
    "assert new_b_hy == b_hy\n",
    "\n",
    "print(f'w_1, w_2, b :: {w_hy_1:.3f}, {w_hy_2:.3f}, {b_hy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30f104",
   "metadata": {},
   "source": [
    "Question 9 : Weight update (continued)\n",
    "---\n",
    "\n",
    "What are the new parameters $W_{x \\rightarrow h}$ and $b_h$ of the first layer?\n",
    "\n",
    "Give your answer using the format *w_11*, *w_12*, *w_21*, *w_22*, *b_1*, *b_2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035e36c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_11, w_12, w_21, w_22, b_1, b_2 :: 0.863, 1.125, 0.870, 1.080, 0.173, -1.335\n"
     ]
    }
   ],
   "source": [
    "Wxh = model.layers[0].weights\n",
    "bh = model.layers[0].biases\n",
    "print(f'w_11, w_12, w_21, w_22, b_1, b_2 :: {Wxh[0,0]:.3f}, {Wxh[0,1]:.3f}, {Wxh[1,0]:.3f}, {Wxh[1,1]:.3f}, {bh[0,0]:.3f}, {bh[0,1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e66c0",
   "metadata": {},
   "source": [
    "Question 10 :  Forward propagation \n",
    "---\n",
    "\n",
    "Now, we will evaluate whether our model has been improved thanks to the back-propagation.\n",
    "\n",
    "Propagate each example $x_i$ through the neural network.\n",
    "\n",
    "How are these examples represented in the hidden space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af4ccce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = model.layers[1].forward(model.layers[0].forward(X_xor))\n",
    "y = np.round(model.forward(X_xor)).reshape(-1)\n",
    "\n",
    "zeros = y == 0\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(r\"$h_1$\")\n",
    "plt.ylabel(r\"$h_2$\")\n",
    "plt.scatter(h[zeros,0], h[zeros,1], color=\"purple\", label=\"0\")\n",
    "plt.scatter(h[~zeros,0], h[~zeros,1], color=\"blue\", label=\"1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c745c",
   "metadata": {},
   "source": [
    "Question 11: Forward propagation (continued) \n",
    "---\n",
    "\n",
    "Are they linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5833170",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb871b3",
   "metadata": {},
   "source": [
    "Question 12: Forward propagation (continued)\n",
    "---\n",
    "\n",
    "What are the values of the outputs $\\hat{y_i}$?\n",
    "\n",
    "Give your answer using the format y_1, y_2, y_3, y_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857bc6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_1, y_2, y_3, y_4 ::  0.4852, 0.6491, 0.6478, 0.3904\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.forward(X_xor)\n",
    "\n",
    "print(\"y_1, y_2, y_3, y_4 :: \", \", \".join([\"%.4f\"%v for v in y_hat.reshape(-1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070519c",
   "metadata": {},
   "source": [
    "Question 13: Classification \n",
    "---\n",
    "\n",
    "How many examples are correctly classified with the updated network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efbb5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(xor_correct(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8b5bb",
   "metadata": {},
   "source": [
    "Question 14: Loss \n",
    "---\n",
    "\n",
    "What is the cross-entropy loss with the updated network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1805df27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :: 2.025\n"
     ]
    }
   ],
   "source": [
    "loss = np.sum(model.loss_function(y_xor, y_hat))\n",
    "print(f\"Loss :: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6401ea1",
   "metadata": {},
   "source": [
    "Question 15: Conclusion \n",
    "---\n",
    "\n",
    "Check all the valid affirmations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc15883",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_xor, y_xor, epochs=1e5, verbose=0)\n",
    "final_y = model.forward(X_xor).reshape(-1)\n",
    "\n",
    "print(\"Final y_hat :: \", \", \".join([\"%.3f\"%v for v in final_y]))\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Crossentropy loss value\")\n",
    "plt.semilogx(history[\"epochs\"], history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b785a42",
   "metadata": {},
   "source": [
    "- [x] The gradient descent algorithm will continue iterating to decrease the loss, until the latter converges\n",
    "- [ ] Since all the examples are correctly classified, the gradient descent algorithm will stop at this point, as the model can no longer be improved\n",
    "- [x] If we let the gradient descent algorithm continue, we could expect it to optimise the network to a point where the output values would be something like $\\hat{y_1}=0.01$, $\\hat{y_2}=0.99$, $\\hat{y_3}=0.99$, $\\hat{y_4}=0.01$  \n",
    "- [ ] The gradient descent algorithm will always converge to the same point, regardless of the initial values of the network parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INGI2262",
   "language": "python",
   "name": "lingi2262"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
