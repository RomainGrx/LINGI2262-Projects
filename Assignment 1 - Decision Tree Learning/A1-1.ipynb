{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quality-munich",
   "metadata": {},
   "source": [
    "Question 1 : ID3 attribute selection\n",
    "---\n",
    "\n",
    "Suppose you have the following training set with four boolean attributes $x_1$, $x_2$, $x_3$ and $x_4$ and a boolean output $y$.\n",
    "\n",
    "![](./imgs/q1_2020.png)\n",
    "\n",
    "What is the tree learned by **ID3** from this training set? You should be able to construct it from your general understanding of this algorithm without going into all the details of computing explicitly every step of this algorithm.\n",
    "\n",
    "### Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "expired-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "feature_names = [\"x1\", \"x2\", \"x3\", \"x4\"]\n",
    "X = [[0, 0, 0, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 1, 1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\") \n",
    "clf.fit(X, y)\n",
    "\n",
    "export_graphviz(clf, \n",
    "                feature_names=feature_names,\n",
    "                filled=True,\n",
    "                out_file=\"A11Q1-decision_tree.dot\")\n",
    "\n",
    "!dot -Tpng A11Q1-decision_tree.dot -o A11Q1-decision_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-agreement",
   "metadata": {},
   "source": [
    "![](./A11Q1-decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-fraction",
   "metadata": {},
   "source": [
    "Question 2: ID3 attribute selection (continued) \n",
    "---\n",
    "\n",
    "Is there another binary decision tree which would perfectly classify the same training examples and would not be as deep as the one proposed by ID3?\n",
    "\n",
    "- [ ] Yes, since ID3 never finds the minimum-depth tree, on any training set, because it is a greedy algorithm (the attribute selection at a node is done without regard to the selection at children nodes).\n",
    "\n",
    "- [x] Yes, for this example, there exists a smaller tree that perfectly classify all training examples. This tree is not found by ID3 since it is a greedy algorithm (the attribute selection at a node is done without regard to the selection at children nodes).\n",
    "\n",
    "- [ ] No, ID3 found the minimum-depth three for this training set, but it is a matter of chance. In general, ID3 does not always find the minimum-depth tree.\n",
    "\n",
    "- [ ] No, ID3 always finds the minimum-depth tree because it maximizes the information gain at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-senior",
   "metadata": {},
   "source": [
    "Question 3: Information gain \n",
    "---\n",
    "\n",
    "Suppose you have a training set with 12 positive and 12 negative examples.\n",
    "\n",
    "Compute the information gain for the two following splits, performed at the root of the tree:\n",
    "- $[10+, 2-]$(left) $[2+, 10-]$(right)\n",
    "- $[12+, 7-]$(left) $[0+, 5-]$(right)\n",
    "\n",
    "Give your answer in the following format: IG_first, IG_second\n",
    "\n",
    "### Answer :\n",
    "\n",
    "```0.349978, 0.24835```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-amount",
   "metadata": {},
   "source": [
    " Question 4: Information gain (continued) \n",
    " ---\n",
    " \n",
    "Based on your answer in the previous question, which split will be chosen by ID3?\n",
    "\n",
    "Beware: you will only receive credit for this question if you answered the previous one correctly.\n",
    "\n",
    "- [x] $[10+, 2-]$(left) $[2+, 10-]$(right)\n",
    "- [ ] $[12+, 7-]$(left) $[0+, 5-]$(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-license",
   "metadata": {},
   "source": [
    "Question 5: Information gain (continued) \n",
    "---\n",
    "\n",
    "Suppose now that the mistakes on the positive examples are about 10 times as costly as mistakes to the negative. One way of dealing with such a cost imbalance is to replicate the positive examples 10 times each. The splits become:\n",
    "\n",
    "- $[100+, 2-]$(left) $[20+, 10-]$(right)\n",
    "- $[120+, 7-]$(left) $[0+, 5-]$(right)\n",
    "\n",
    "What is their information gain?\n",
    "\n",
    "Give your answer in the following format: IG_first, IG_second\n",
    "\n",
    "### Answer :\n",
    "\n",
    "```0.123204,0.143402```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-constitution",
   "metadata": {},
   "source": [
    "Question 6: Information gain (continued) \n",
    "---\n",
    "\n",
    "Consequently, which split will be performed by ID3?\n",
    "\n",
    "Beware: you will only receive credit for this question if you answered the previous one correctly.\n",
    "\n",
    "- [ ] $[100+, 2-]$(left) $[20+, 10-]$(right)\n",
    "- [x] $[120+, 7-]$(left) $[0+, 5-]$(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-cooking",
   "metadata": {},
   "source": [
    "Question 7: Information gain (continued) \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-baghdad",
   "metadata": {},
   "source": [
    "Question 8: Search space \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-scope",
   "metadata": {},
   "source": [
    "Question 9: C4.5 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-appraisal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lingi2262",
   "language": "python",
   "name": "lingi2262"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
