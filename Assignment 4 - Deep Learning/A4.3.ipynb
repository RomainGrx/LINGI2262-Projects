{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5979e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import threading\n",
    "from threading import Thread\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "((X_train, y_train), (X_test, y_test)) = keras.datasets.fashion_mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "def config_single_gpu(gpu):\n",
    "    if isinstance(gpu, int):\n",
    "        gpu = tf.config.list_physical_devices(\"GPU\")[gpu]\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "\n",
    "class AttrDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "        \n",
    "class Simulation:\n",
    "    def __init__(self, lr, layers_params):\n",
    "        self.lr = lr\n",
    "        self.layers_params = layers_params\n",
    "    \n",
    "    def get_model(self):\n",
    "        layers = [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        ]\n",
    "        for params in self.layers_params:\n",
    "            layers.append(\n",
    "                tf.keras.layers.Dense(params.n, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"tanh\")\n",
    "            )\n",
    "        layers.append(\n",
    "            tf.keras.layers.Dense(10, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"softmax\")\n",
    "        )\n",
    "        model = tf.keras.models.Sequential(layers)\n",
    "        model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=self.lr),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def run(self, train_ds, test_ds, runs, epochs, batch_size, output=[]):\n",
    "        def fit_worker(*args):\n",
    "            model = self.get_model()\n",
    "            fit_feedback = model.fit(train_ds,\n",
    "                                     validation_data=test_ds,\n",
    "                                     batch_size=batch_size, \n",
    "                                     epochs=epochs,\n",
    "                                     use_multiprocessing=True,\n",
    "                                     verbose=0)\n",
    "            acc =  fit_feedback.history[\"val_accuracy\"][-1]\n",
    "            return acc\n",
    "        \n",
    "        with ThreadPool() as p:\n",
    "            acc = p.map(fit_worker, range(runs))\n",
    "            \n",
    "        out = tf.reduce_mean(acc).numpy()\n",
    "        output.append(out)\n",
    "\n",
    "class ThreadSimulations:\n",
    "    def __init__(self, lr, layers_params):\n",
    "        self.lr = lr\n",
    "        self.layers_params = layers_params\n",
    "        self.simulations = [Simulation(lr, lp) for lp in self.layers_params]\n",
    "    \n",
    "    def run(self, train_ds, test_ds, runs, epochs, batch_size, mean=True):\n",
    "        outputs = [[] for _ in self.simulations]\n",
    "        threads = [\n",
    "            Thread(target=sim.run, args=(train_ds, test_ds, runs, epochs, batch_size, out)) for sim, out in zip(self.simulations, outputs)\n",
    "        ]\n",
    "        \n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        \n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        if mean:\n",
    "            return [tf.reduce_mean(out).numpy() for out in outputs]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbd1ad",
   "metadata": {},
   "source": [
    "Question 1 : Hidden units\n",
    "---\n",
    "\n",
    "Take again the neural network you defined in question 4 of the previous task (one hidden layer with *tanh* activation, with a learning rate of $10^{-5}$\n",
    "\n",
    "). Let's study the impact of the number of units in the hidden layer.\n",
    "\n",
    "Build a model with 10 units in the hidden layer, one with $100$ units, and another one with $1000$ units. Which one performs best ?\n",
    "\n",
    "Perform $10$ distinct runs (training + testing) for each model and average the results. Use $100$ epochs to fit each model.\n",
    "\n",
    "Don't change anything in your network besides the number of hidden units.\n",
    "\n",
    "Report the mean test accuracies of the three models using the format: *test_acc_10*, *test_acc_100*, *test_acc_1000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e139d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc_10, test_acc_100, test_acc_1000 ::  0.743, 0.839, 0.867\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 10\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "n_hiddens = [10, 100, 1000]\n",
    "\n",
    "config_single_gpu(1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().batch(BATCH_SIZE).prefetch(-1)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().batch(BATCH_SIZE).prefetch(-1)\n",
    "\n",
    "layers_params = [[AttrDict(n=n)] for n in n_hiddens]\n",
    "\n",
    "single_layer_simu = ThreadSimulations(1e-5, layers_params)\n",
    "means = single_layer_simu.run(train_ds, test_ds, N_RUNS, N_EPOCHS, BATCH_SIZE)\n",
    "\n",
    "print(f\"test_acc_10, test_acc_100, test_acc_1000 :: \", \", \".join([f\"{v:.3f}\" for v in means]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3950bfd",
   "metadata": {},
   "source": [
    "Question 2 : Adding layers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a4ba386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "n_hiddens = [10, 100, 1000]\n",
    "n_layers = [1, 2, 3]\n",
    "\n",
    "layers_params = []\n",
    "for n_layer in n_layers:\n",
    "    combi = list(product(n_hiddens, repeat=n_layer))\n",
    "    for params in combi:\n",
    "        layers_params.append(\n",
    "            [AttrDict(n=n) for n in params]\n",
    "        )\n",
    "\n",
    "multi_layer_simu = ThreadSimulations(1e-5, layers_params)\n",
    "means = multi_layer_simu.run(x_train, y_train, x_test, y_test, N_RUNS, N_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ce30c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for layers :: \n",
      " 1000 1000\n"
     ]
    }
   ],
   "source": [
    "best_mean_idx = tf.argmax(means).numpy()\n",
    "print(\"Best params for layers :: \", *[a.n for a in layers_params[best_mean_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dce4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/romaingrx/Nextcloud/EPL/Q8/Cours/LINGI2262 - Machine Learning/LINGI2262-Projects/Assignment 4 - Deep Learning\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingi2261",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
